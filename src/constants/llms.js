export const MARKDOWN_CONTENTS = [
  `

## GPT Stonks LLM Integration
![HuggingFace Badge](https://img.shields.io/badge/HuggingFace-Supports-green)
![OpenBB Badge](https://img.shields.io/badge/OpenBB-Platform-red)
![GptStonks Badge](https://img.shields.io/badge/GptStonks-Info-yellow)

### ü§ñ What is an LLM?

LLM stands for Language Model. It's a type of machine learning model designed to understand and generate human-like text based on the input it receives.

### üõ† How does it work?

LLMs, especially those like GPT-3, are trained on vast amounts of text data. They predict the next word in a sequence, which allows them to generate coherent and contextually relevant sentences.

### üîÑ How can affect to choose between one LLM and another one in generative responses?

Choosing between different LLMs can significantly impact the quality, style, and context of the generated responses. Each model might be trained on different datasets, have varying sizes, and be optimized for specific tasks.

### üöÄ Potential difference in performance

Using APIs like ![OpenAI Badge](https://img.shields.io/badge/OpenAI-API-blue) or ![Anthropic Badge](https://img.shields.io/badge/Anthropic-API-orange) usually provides access to the latest models without needing the computational resources to run them. However, running an LLM like Griffin on a local GPU can offer more control, privacy, and potentially lower latency, but requires significant computational power and proper setup.

`,
  `

## Overview of Current LLMs in the Market
![HuggingFace Badge](https://img.shields.io/badge/HuggingFace-Supports-green)
![OpenBB Badge](https://img.shields.io/badge/OpenBB-Platform-red)
![GptStonks Badge](https://img.shields.io/badge/GptStonks-Info-yellow)

### üåê Landscape of LLMs

The landscape of Language Learning Models has expanded dramatically over the past few years. Companies and research institutions are continually pushing the boundaries, resulting in more advanced and capable models.

### üöÄ Leading LLMs in the Market

- **OpenAI's GPT Series**: This series, including the famous GPT-3, set a benchmark in the NLP community. They offer excellent text generation capabilities and have wide applications.
- **Google's BERT and T5**: These models are optimized for understanding the context of words in search queries. They have revolutionized search engine performance and understanding.
- **Facebook's BART and RoBERTa**: These models are designed for tasks that require understanding the entire context, making them powerful tools for summarization and translation.

### üåü Unique Features & Advantages

Different LLMs come with their own set of advantages. For instance, some are designed for high performance in specific tasks like translation or summarization, while others are generalized models suitable for a range of applications. The choice of model can often depend on the specific use case, available computational resources, and desired output quality.

`,
  `

## Cost Implications of Using LLMs
![HuggingFace Badge](https://img.shields.io/badge/HuggingFace-Supports-green)
![OpenBB Badge](https://img.shields.io/badge/OpenBB-Platform-red)
![GptStonks Badge](https://img.shields.io/badge/GptStonks-Info-yellow)

### üí∞ Tokens and Cost

The cost of using an LLM can vary based on the number of tokens processed. Each word or piece of punctuation is typically considered a token. Longer texts consume more tokens, resulting in higher costs.

### üìä Model Complexity and Cost

Different models come with varying complexities. Using a more complex and larger model can produce better results but at a higher computational cost. For instance, a top-tier model might produce excellent text but might be expensive to run, especially for lengthy inputs.

### üï∞ Real-time vs. Batch Processing

Real-time processing, where you seek immediate responses, can be costlier than batch processing, where you process large amounts of data at once. The choice between real-time and batch can also influence the overall cost.

### üöÄ Optimizing Costs

To manage costs, users can:
- Opt for smaller models when high accuracy is not critical.
- Use batching to process data.
- Monitor token usage to avoid overages.

Understanding the balance between cost, performance, and requirements is essential to effectively use LLMs without incurring unnecessary expenses.

`,
];
